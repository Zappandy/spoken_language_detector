{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mel Spectrogram CNN Classifier Demo\n",
    "\n",
    "This is a demo on constructing a CNN classifier to classify speech as one of two words (zero or one) based on a training set of Mel spectra (images) obtained from speech. We shall be using the same Librosa library as we did in other demos to obtain the Mel spectra from wav files.\n",
    "\n",
    "Our dataset will consist of two folders, one with 1000 speech samples of the word 'one', and another with 1000 samples of the word 'zero'. The first thing we have to do is to pre-process the wav files in order to generate spectrographic data for all the files. We will be using the same code in the Mel spectrum generation demo for this purpose.\n",
    "\n",
    "Starting with some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need a constant for the frame size for Mel spectrogram processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "mel_spec_frame_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall define some useful functions that will help our preprocessing stage. The first function scans the data folders we have to determine how many classes are present. We have two subfolders, one per class. The following function returns the subfolder list, as well as the associated label for each of these classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_classes_in_datapath(datapath='./data_notebook_2'):\n",
    "    \"\"\"\n",
    "    Returns a list of sub-folders in the data path, assuming each subfolder is a separate class. Each sub-folder\n",
    "    is associated with a numeric class label, which is also returned.\n",
    "    :param datapath: Main data path\n",
    "    :return: Tuple of individual class folder paths, and the corresponding numeric class label for each folder\n",
    "    \"\"\"\n",
    "    subfolders = [f.path for f in os.scandir(datapath) if f.is_dir()]\n",
    "    class_labels = np.arange(0,len(subfolders))\n",
    "    return subfolders, class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function we need, is to return a list of audio files for a particular folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_wav_files_in_path(datapath):\n",
    "    \"\"\"\n",
    "    Returns the list of .wav files in a directory.\n",
    "    :param datapath: Directory to search for wav files in.\n",
    "    :return: List of paths to wav files.\n",
    "    \"\"\"\n",
    "    files = os.listdir(datapath)\n",
    "    files_wav = [i for i in files if i.endswith('.wav')]\n",
    "    return files_wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One function you should be familiar with is to get a Mel spectrogram from an audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def get_melspec_from_wav(wavfile, n_mels=64, plot=False):\n",
    "    \"\"\"\n",
    "    Given a path to a wav file, returns a melspectrogram array.\n",
    "    np.ndarray [shape=(n_mels, t)]\n",
    "    :param wavfile: The input wav file.\n",
    "    :param n_mels: The number of mel spectrogram filters.\n",
    "    :param plot: Flag to either plot the spectorgram or not.\n",
    "    :return: Returns a tuple of np.ndarray [shape=(n_mels, t)] and fs\n",
    "    \"\"\"\n",
    "    sig, fs = librosa.load(wavfile,sr=None)\n",
    "\n",
    "    # Normalize audio to between -1.0 and +1.0\n",
    "    sig /= np.max(np.abs(sig), axis=0)\n",
    "\n",
    "    if len(sig) < fs: # pad if less than a second\n",
    "        shape = np.shape(sig)\n",
    "        padded_array = np.zeros(fs)\n",
    "        padded_array[:shape[0]] = sig\n",
    "        sig = padded_array\n",
    "\n",
    "    melspec = librosa.feature.melspectrogram(y=sig,\n",
    "                                             sr=fs,\n",
    "                                             center=True,\n",
    "                                             n_fft=mel_spec_frame_size,\n",
    "                                             hop_length=int(mel_spec_frame_size/2),\n",
    "                                             n_mels=n_mels)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Mel-Frequency')\n",
    "        librosa.display.specshow(librosa.power_to_db(melspec, ref=np.max),\n",
    "                                 y_axis='mel',\n",
    "                                 fmax=fs/2,\n",
    "                                 sr=fs,\n",
    "                                 hop_length=int(mel_spec_frame_size / 2),\n",
    "                                 x_axis='time')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Mel spectrogram')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return melspec, fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a way to export the Mel spectra and not have to process the training set every time we change our CNN architecture. We can do this in two ways: one is to export the raw spectrum values in a compressed numpy file (NPZ) - which can be done with a numpy call. The other method (and this will be used just for viewing images easily) will be to export the actual image as a PNG file with this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def save_image(filepath, fig=None):\n",
    "    '''Save the current image with no whitespace\n",
    "    Example filepath: \"myfig.png\" or r\"C:\\myfig.pdf\"\n",
    "    '''\n",
    "    if not fig:\n",
    "        fig = plt.gcf()\n",
    "\n",
    "    plt.subplots_adjust(0,0,1,1,0,0)\n",
    "    for ax in fig.axes:\n",
    "        ax.axis('off')\n",
    "        ax.margins(0,0)\n",
    "        ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "        ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "    fig.savefig(filepath, pad_inches = 0, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the basic building blocks to run a data pre-processing routine to:\n",
    "1) Get a list of classes in our dataset\n",
    "\n",
    "2) For each class, get a list of wav files\n",
    "\n",
    "3) For each wav file, get a normalized Mel spectrogram\n",
    "\n",
    "4) Save the Mel spectrogram as an NPZ file and as a PNG file\n",
    "\n",
    "We shall be saving the NPZ and PNG files adjacent to the WAV files, with the same filename, but different extensions. The function below does all of the above will take some time to execute for our 2000 files, but once completed, we can re-use the data for our CNN experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    \"\"\"\n",
    "    Performs initial data preprocessing - extracting Mel spectra for all files and padding them appropriately\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    subfolders, class_labels = get_classes_in_datapath()\n",
    "\n",
    "    for folder in subfolders:\n",
    "        files_wav = get_wav_files_in_path(datapath=folder)\n",
    "        for file in files_wav:\n",
    "            # get melspec\n",
    "            melspec, fs = get_melspec_from_wav(wavfile=os.path.join(folder, file),\n",
    "                                               plot=False,\n",
    "                                               n_mels=64)\n",
    "\n",
    "            melspec = librosa.power_to_db(melspec, ref=1.0)\n",
    "            melspec = melspec / 80.0 # scale by max dB\n",
    "\n",
    "            #check we have 64 time samples (and pad)\n",
    "            if melspec.shape[1] < 64:\n",
    "                shape = np.shape(melspec)\n",
    "                padded_array = np.zeros((shape[0],64))-1\n",
    "                padded_array[0:shape[0],:shape[1]] = melspec\n",
    "                melspec = padded_array\n",
    "\n",
    "            # save melspec\n",
    "            melspec_filename = (os.path.join(folder, file)).replace('.wav', '.mel')\n",
    "            np.savez(melspec_filename, melspec=melspec, fs=fs)\n",
    "\n",
    "            # save melspec image\n",
    "            melspec_image_filename = (os.path.join(folder, file)).replace('.wav', '.png')\n",
    "            fig = plt.figure(figsize=(10, 4))\n",
    "            plt.imshow(melspec, origin='lower')\n",
    "            plt.tight_layout()\n",
    "            save_image(filepath=melspec_image_filename,fig=fig)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the function and let it run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pre-processing...\n",
      "Pre-processing finished.\n"
     ]
    }
   ],
   "source": [
    "do_preprocessing = False  # change to true if preprocessing has not been performed.\n",
    "\n",
    "print('Starting pre-processing...')\n",
    "if do_preprocessing:\n",
    "    preprocessing()\n",
    "print('Pre-processing finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Classifier Construction\n",
    "We shall now create a Python class that will construct and execute a CNN classifier for our Mel spectrogram data. Let's start by the class definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import ReLU, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class SpeechCNNClassifier():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 64\n",
    "        self.img_cols = 64\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        # Scan Data Set, shuffle it, and split into training/validation\n",
    "        images, labels = self.get_trainingset()\n",
    "        images, labels = shuffle(images, labels)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = \\\n",
    "            train_test_split(images, labels, test_size = 0.33, random_state = 42)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        optimizer = Adam(learning_rate=0.0001)\n",
    "        self.classifier = self.build_classifier()\n",
    "        self.classifier.compile(loss='binary_crossentropy',\n",
    "                                optimizer=optimizer,\n",
    "                                metrics=['binary_accuracy', 'categorical_accuracy'])\n",
    "\n",
    "    def get_trainingset(self):\n",
    "        \"\"\"\n",
    "        Returns a list of training data files (NPZ files) and their respective labels\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        def get_npz_files_in_path(datapath):\n",
    "            files = os.listdir(datapath)\n",
    "            files_npz = [i for i in files if i.endswith('.npz')]\n",
    "            return files_npz\n",
    "\n",
    "        training_images = []\n",
    "        labels = []\n",
    "\n",
    "        subfolders, class_labels = get_classes_in_datapath()\n",
    "\n",
    "        class_idx = 0\n",
    "        for folder in subfolders:\n",
    "            label = class_labels[class_idx]\n",
    "            files_mel = get_npz_files_in_path(datapath=folder)\n",
    "\n",
    "            temp_labels = np.empty(shape=(len(files_mel)),dtype=int)\n",
    "            temp_labels[:] = label\n",
    "            labels.extend(temp_labels)\n",
    "            class_idx += 1\n",
    "\n",
    "            for file in files_mel:\n",
    "                training_images.append((os.path.join(folder, file)))\n",
    "\n",
    "        return training_images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructor above contains a number of definitions that will come in handy. The first parameters define the size of the image in terms of rows, columns and images. We will be working with 64x64 pixel images (the Mel spectra). Each image contains just one channel of information. These values define the image shape. Furthermore, we shall be supplying the classifier with a training set, which is a list of image file paths and their respective label. This will be done with a method called get_trainingset, which scans our directory structure and prepares the dataset.\n",
    "\n",
    "Furthermore, these labels are also one-hot encoded, for categorical classification purposes. We then split the data into training/validation with around 1/3 of the data kept for validation.\n",
    "\n",
    "Furthemore, the constructor also creates the CNN compilation for us. This consists of:\n",
    "1) defining an optimization function. We shall be using an Adam optimizer, with a learning rate of 0.0001. Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems. Most CNNs today make use of this optimizer.\n",
    "\n",
    "2) We shall call an internal method that builds the classifier (CNN) for us. We shall be discussing this method soon. This method will be the main place where the architecture of the CNN is defined/modified.\n",
    "\n",
    "3) We combine the constructed CNN and the optimizer into a compilation that works to minimize binary cross-entropy loss. A very good explanation of this loss function can be found here: https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
    "\n",
    "Now let us define our CNN architecture, the nuts and bolts of this class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def build_classifier(self):\n",
    "    \"\"\"\n",
    "    Defines the classifier network.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, kernel_size=3, strides=1, padding='same', input_shape=self.img_shape))\n",
    "    model.add(ReLU())\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, padding='same'))\n",
    "    model.add(ReLU())\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding='same'))\n",
    "    model.add(ReLU())\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
    "    model.add(ReLU())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN is a sequential structured network, made up of 4 convolutional blocks. Each convolutional block will have a ReLU activation function. The convolutional layers all have a 3x3 kernel size. The strides vary between 1 or 2 across the different layers. The number of filters in each convolution block increase twice at every layer, starting from 16, going up to 128.\n",
    "\n",
    "After the convolutional blocks, the CNN is flattened into a 1-D layer, a dropout is applied, and an output sigmoid layer with one node, which will give us a continuous value between 0 and 1. We will assume the proximity to 0 and 1 to be an indicator of classification of zero or one images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now need to define our training loop. Since we are dealing with quite a lot of data, we shall not use the typical Keras tutorial approach of loading all data in memory and running a 'fit' with defined epochs and batch size. Instead, we shall customize the way data is fed into the CNN for training.\n",
    "\n",
    "We need to train our CNN over a number of epochs. An epoch is defined as a full pass of the CNN over the training data. Since we will be training our CNN with a particular batch size, we need to calculate how many steps are to be executed in our epoch (dataset size / batch size). We then prepare our validation dataset in memory, as we want to run a validation test at the end of every epoch.\n",
    "\n",
    "Training then takes places, by looping for the number of epochs required, and for all the steps in an epoch. The batch of data required for a particular step is loaded in memory (read from NPZ files) with associated labels. The training batch is passed on to the CNN for a batch update. When all steps in an epoch are complete, the validation metrics are calculated. Training completes when all epochs have been executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(self, epochs, batch_size=64):\n",
    "    # steps per epoch\n",
    "    steps_per_epoch = int(len(self.X_train)/batch_size)\n",
    "\n",
    "    # prepare validation set images and labels\n",
    "    validation_imgs = []\n",
    "    for file_path in self.X_test:\n",
    "        npzfile = np.load(file_path)\n",
    "        melspec = npzfile['melspec']\n",
    "        validation_imgs.append(melspec)\n",
    "    validation_imgs = np.asarray(validation_imgs)\n",
    "    validation_imgs = np.expand_dims(validation_imgs, axis=3)\n",
    "    validation_lbls = np.asarray(self.y_test)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for step in range(steps_per_epoch):\n",
    "            # ---------------------\n",
    "            #  Train CNN\n",
    "            # ---------------------\n",
    "\n",
    "            # Select next batch of images (and shuffle indexes)\n",
    "            idx = np.arange(step*batch_size,(step*batch_size)+batch_size)\n",
    "            imgs = []\n",
    "            for file_path in np.asarray(self.X_train)[idx]:\n",
    "                npzfile = np.load(file_path)\n",
    "                melspec = npzfile['melspec']\n",
    "                imgs.append(melspec)\n",
    "            imgs = np.asarray(imgs)\n",
    "            imgs = np.expand_dims(imgs, axis=3)\n",
    "            lbls = np.asarray(self.y_train)[idx]\n",
    "\n",
    "            # batch-train CNN\n",
    "            loss, accuracy = self.classifier.train_on_batch(imgs,lbls)\n",
    "            # Output the progress\n",
    "            print(\"Epoch: (%d/%d) Step: (%d/%d) [Loss: %f, Accuracy: %f]\"\n",
    "                % (epoch, epochs - 1, step, steps_per_epoch - 1, loss, accuracy))\n",
    "\n",
    "        # After all steps in this epoch are complete, run validation\n",
    "        v_loss, v_accuracy = self.classifier.evaluate(x=validation_imgs,y=validation_lbls)\n",
    "        # Output the progress\n",
    "        print(\"Epoch: (%d/%d) Validation [Loss: %f, Accuracy: %f]\"\n",
    "              % (epoch, epochs - 1, v_loss, v_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete CNN class is defined here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SpeechCNNClassifier():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 64\n",
    "        self.img_cols = 64\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        # Scan Data Set, shuffle it, and split into training/validation\n",
    "        images, labels = self.get_trainingset()\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_encoded = label_encoder.fit_transform(labels)\n",
    "        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "        labels = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "        images, labels = shuffle(images, labels)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = \\\n",
    "            train_test_split(images, labels, test_size = 0.33, random_state = 42)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        optimizer = Adam(learning_rate=0.0001)\n",
    "        self.classifier = self.build_classifier()\n",
    "        self.classifier.compile(loss='categorical_crossentropy',\n",
    "                                        optimizer=optimizer,\n",
    "                                        metrics=['accuracy'])\n",
    "\n",
    "    def get_trainingset(self):\n",
    "        \"\"\"\n",
    "        Returns a list of training data files (NPZ files) and their respective labels\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        def get_npz_files_in_path(datapath):\n",
    "            files = os.listdir(datapath)\n",
    "            files_npz = [i for i in files if i.endswith('.npz')]\n",
    "            return files_npz\n",
    "\n",
    "        training_images = []\n",
    "        labels = []\n",
    "\n",
    "        subfolders, class_labels = get_classes_in_datapath()\n",
    "\n",
    "        class_idx = 0\n",
    "        for folder in subfolders:\n",
    "            label = class_labels[class_idx]\n",
    "            files_mel = get_npz_files_in_path(datapath=folder)\n",
    "\n",
    "            temp_labels = np.empty(shape=(len(files_mel)),dtype=int)\n",
    "            temp_labels[:] = label\n",
    "            labels.extend(temp_labels)\n",
    "            class_idx += 1\n",
    "\n",
    "            for file in files_mel:\n",
    "                training_images.append((os.path.join(folder, file)))\n",
    "\n",
    "        return training_images, labels\n",
    "\n",
    "    def build_classifier(self):\n",
    "        \"\"\"\n",
    "        Defines the classifier network.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(16, kernel_size=3, strides=1, padding='same', input_shape=self.img_shape))\n",
    "        model.add(ReLU())\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, padding='same'))\n",
    "        model.add(ReLU())\n",
    "\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding='same'))\n",
    "        model.add(ReLU())\n",
    "\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
    "        model.add(ReLU())\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self, epochs, batch_size=64):\n",
    "        # steps per epoch\n",
    "        steps_per_epoch = int(len(self.X_train)/batch_size)\n",
    "\n",
    "        # prepare validation set images and labels\n",
    "        validation_imgs = []\n",
    "        for file_path in self.X_test:\n",
    "            npzfile = np.load(file_path)\n",
    "            melspec = npzfile['melspec']\n",
    "            validation_imgs.append(melspec)\n",
    "        validation_imgs = np.asarray(validation_imgs)\n",
    "        validation_imgs = np.expand_dims(validation_imgs, axis=3)\n",
    "        validation_lbls = np.asarray(self.y_test)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for step in range(steps_per_epoch):\n",
    "                # ---------------------\n",
    "                #  Train CNN\n",
    "                # ---------------------\n",
    "\n",
    "                # Select next batch of images (and shuffle indexes)\n",
    "                idx = np.arange(step*batch_size,(step*batch_size)+batch_size)\n",
    "                imgs = []\n",
    "                for file_path in np.asarray(self.X_train)[idx]:\n",
    "                    npzfile = np.load(file_path)\n",
    "                    melspec = npzfile['melspec']\n",
    "                    imgs.append(melspec)\n",
    "                imgs = np.asarray(imgs)\n",
    "                imgs = np.expand_dims(imgs, axis=3)\n",
    "                lbls = np.asarray(self.y_train)[idx]\n",
    "\n",
    "                # batch-train CNN\n",
    "                loss, accuracy = self.classifier.train_on_batch(imgs,lbls)\n",
    "                # Output the progress, uncomment to see\n",
    "#                 print(\"Epoch: (%d/%d) Step: (%d/%d) [Loss: %f, Accuracy: %f]\"\n",
    "#                     % (epoch, epochs - 1, step, steps_per_epoch - 1, loss, accuracy))\n",
    "\n",
    "            # After all steps in this epoch are complete, run validation\n",
    "            v_loss, v_accuracy = self.classifier.evaluate(x=validation_imgs,y=validation_lbls)\n",
    "            # Output the progress\n",
    "            print(\"Epoch: (%d/%d) Validation [Loss: %f, Accuracy: %f]\"\n",
    "                  % (epoch, epochs - 1, v_loss, v_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it a go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 12:16:23.048771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-21 12:16:24.478951: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-04-21 12:16:24.478998: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-21 12:16:24.525844: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 16)        160       \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 64, 64, 16)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)        4640      \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 16386     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 113,538\n",
      "Trainable params: 113,538\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "21/21 [==============================] - 2s 25ms/step - loss: 0.6745 - accuracy: 0.6833\n",
      "Epoch: (0/24) Validation [Loss: 0.674534, Accuracy: 0.683333]\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.6263 - accuracy: 0.7197\n",
      "Epoch: (1/24) Validation [Loss: 0.626304, Accuracy: 0.719697]\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.5299 - accuracy: 0.7818\n",
      "Epoch: (2/24) Validation [Loss: 0.529948, Accuracy: 0.781818]\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.4199 - accuracy: 0.8682\n",
      "Epoch: (3/24) Validation [Loss: 0.419921, Accuracy: 0.868182]\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.3153 - accuracy: 0.8970\n",
      "Epoch: (4/24) Validation [Loss: 0.315349, Accuracy: 0.896970]\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.2459 - accuracy: 0.9121\n",
      "Epoch: (5/24) Validation [Loss: 0.245940, Accuracy: 0.912121]\n",
      "21/21 [==============================] - 0s 20ms/step - loss: 0.2005 - accuracy: 0.9333\n",
      "Epoch: (6/24) Validation [Loss: 0.200473, Accuracy: 0.933333]\n",
      "21/21 [==============================] - 0s 16ms/step - loss: 0.1734 - accuracy: 0.9439\n",
      "Epoch: (7/24) Validation [Loss: 0.173355, Accuracy: 0.943939]\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.1591 - accuracy: 0.9455\n",
      "Epoch: (8/24) Validation [Loss: 0.159063, Accuracy: 0.945455]\n",
      "21/21 [==============================] - 0s 18ms/step - loss: 0.1464 - accuracy: 0.9500\n",
      "Epoch: (9/24) Validation [Loss: 0.146375, Accuracy: 0.950000]\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.1372 - accuracy: 0.9530\n",
      "Epoch: (10/24) Validation [Loss: 0.137217, Accuracy: 0.953030]\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.1283 - accuracy: 0.9636\n",
      "Epoch: (11/24) Validation [Loss: 0.128301, Accuracy: 0.963636]\n",
      "21/21 [==============================] - 0s 14ms/step - loss: 0.1200 - accuracy: 0.9652\n",
      "Epoch: (12/24) Validation [Loss: 0.120012, Accuracy: 0.965151]\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.1190 - accuracy: 0.9652\n",
      "Epoch: (13/24) Validation [Loss: 0.118968, Accuracy: 0.965151]\n",
      "21/21 [==============================] - 0s 19ms/step - loss: 0.1107 - accuracy: 0.9712\n",
      "Epoch: (14/24) Validation [Loss: 0.110729, Accuracy: 0.971212]\n",
      "21/21 [==============================] - 0s 12ms/step - loss: 0.1046 - accuracy: 0.9742\n",
      "Epoch: (15/24) Validation [Loss: 0.104597, Accuracy: 0.974242]\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.1039 - accuracy: 0.9712\n",
      "Epoch: (16/24) Validation [Loss: 0.103853, Accuracy: 0.971212]\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.0980 - accuracy: 0.9742\n",
      "Epoch: (17/24) Validation [Loss: 0.097997, Accuracy: 0.974242]\n",
      "21/21 [==============================] - 0s 19ms/step - loss: 0.0956 - accuracy: 0.9742\n",
      "Epoch: (18/24) Validation [Loss: 0.095588, Accuracy: 0.974242]\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.0917 - accuracy: 0.9727\n",
      "Epoch: (19/24) Validation [Loss: 0.091692, Accuracy: 0.972727]\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.0917 - accuracy: 0.9712\n",
      "Epoch: (20/24) Validation [Loss: 0.091733, Accuracy: 0.971212]\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.0859 - accuracy: 0.9742\n",
      "Epoch: (21/24) Validation [Loss: 0.085890, Accuracy: 0.974242]\n",
      "21/21 [==============================] - 0s 19ms/step - loss: 0.0878 - accuracy: 0.9727\n",
      "Epoch: (22/24) Validation [Loss: 0.087820, Accuracy: 0.972727]\n",
      "21/21 [==============================] - 0s 13ms/step - loss: 0.0802 - accuracy: 0.9773\n",
      "Epoch: (23/24) Validation [Loss: 0.080230, Accuracy: 0.977273]\n",
      "21/21 [==============================] - 0s 17ms/step - loss: 0.0814 - accuracy: 0.9742\n",
      "Epoch: (24/24) Validation [Loss: 0.081397, Accuracy: 0.974242]\n"
     ]
    }
   ],
   "source": [
    "# Build CNN\n",
    "cnn = SpeechCNNClassifier()\n",
    "\n",
    "# sTrain CNN\n",
    "batch_size = 32\n",
    "cnn.train(epochs=25, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Homework\n",
    "\n",
    "You should have a go at modifying this notebook in order to achieve the following:\n",
    "\n",
    "1) Plot loss and accuracy curves at the end of epoch training. This will allow you to diagnose your CNNs performance and training, and possibly identify problems.\n",
    "\n",
    "2) Train the system for more epochs. Does performance improve? When do improvements reach a plateau? \n",
    "\n",
    "Plateaus at around 21 epochs\n",
    "\n",
    "3) Modify the CNN architecture to try and achieve better classification results on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SpeechCNNClassifier():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 64\n",
    "        self.img_cols = 64\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        # Scan Data Set, shuffle it, and split into training/validation\n",
    "        images, labels = self.get_trainingset()\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_encoded = label_encoder.fit_transform(labels)\n",
    "        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "        labels = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "        images, labels = shuffle(images, labels)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = \\\n",
    "            train_test_split(images, labels, test_size = 0.33, random_state = 42)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        optimizer = Adam(learning_rate=0.0001)\n",
    "        self.classifier = self.build_classifier()\n",
    "        self.classifier.compile(loss='categorical_crossentropy',\n",
    "                                        optimizer=optimizer,\n",
    "                                        metrics=['accuracy'])\n",
    "\n",
    "    def get_trainingset(self):\n",
    "        \"\"\"\n",
    "        Returns a list of training data files (NPZ files) and their respective labels\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        def get_npz_files_in_path(datapath):\n",
    "            files = os.listdir(datapath)\n",
    "            files_npz = [i for i in files if i.endswith('.npz')]\n",
    "            return files_npz\n",
    "\n",
    "        training_images = []\n",
    "        labels = []\n",
    "\n",
    "        subfolders, class_labels = get_classes_in_datapath()\n",
    "\n",
    "        class_idx = 0\n",
    "        for folder in subfolders:\n",
    "            label = class_labels[class_idx]\n",
    "            files_mel = get_npz_files_in_path(datapath=folder)\n",
    "\n",
    "            temp_labels = np.empty(shape=(len(files_mel)),dtype=int)\n",
    "            temp_labels[:] = label\n",
    "            labels.extend(temp_labels)\n",
    "            class_idx += 1\n",
    "\n",
    "            for file in files_mel:\n",
    "                training_images.append((os.path.join(folder, file)))\n",
    "\n",
    "        return training_images, labels\n",
    "\n",
    "    def build_classifier(self):\n",
    "        \"\"\"\n",
    "        Defines the classifier network.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(16, kernel_size=3, strides=1, padding='same', input_shape=self.img_shape))\n",
    "        model.add(ReLU())\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, padding='same'))\n",
    "        model.add(ReLU())\n",
    "\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding='same'))\n",
    "        model.add(ReLU())\n",
    "\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
    "        model.add(ReLU())\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self, epochs, batch_size=64):\n",
    "        # steps per epoch\n",
    "        steps_per_epoch = int(len(self.X_train)/batch_size)\n",
    "\n",
    "        # prepare validation set images and labels\n",
    "        validation_imgs = []\n",
    "        for file_path in self.X_test:\n",
    "            npzfile = np.load(file_path)\n",
    "            melspec = npzfile['melspec']\n",
    "            validation_imgs.append(melspec)\n",
    "        validation_imgs = np.asarray(validation_imgs)\n",
    "        validation_imgs = np.expand_dims(validation_imgs, axis=3)\n",
    "        validation_lbls = np.asarray(self.y_test)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for step in range(steps_per_epoch):\n",
    "                # ---------------------\n",
    "                #  Train CNN\n",
    "                # ---------------------\n",
    "\n",
    "                # Select next batch of images (and shuffle indexes)\n",
    "                idx = np.arange(step*batch_size,(step*batch_size)+batch_size)\n",
    "                imgs = []\n",
    "                for file_path in np.asarray(self.X_train)[idx]:\n",
    "                    npzfile = np.load(file_path)\n",
    "                    melspec = npzfile['melspec']\n",
    "                    imgs.append(melspec)\n",
    "                imgs = np.asarray(imgs)\n",
    "                imgs = np.expand_dims(imgs, axis=3)\n",
    "                lbls = np.asarray(self.y_train)[idx]\n",
    "\n",
    "                # batch-train CNN\n",
    "                loss, accuracy = self.classifier.train_on_batch(imgs, lbls)\n",
    "                # for plotting\n",
    "                history = self.classifier.fit(imgs, lbls, epochs=epochs)\n",
    "                # Output the progress, uncomment to see\n",
    "#                 print(\"Epoch: (%d/%d) Step: (%d/%d) [Loss: %f, Accuracy: %f]\"\n",
    "#                     % (epoch, epochs - 1, step, steps_per_epoch - 1, loss, accuracy))\n",
    "\n",
    "            # After all steps in this epoch are complete, run validation\n",
    "            v_loss, v_accuracy = self.classifier.evaluate(x=validation_imgs,y=validation_lbls)\n",
    "            # Output the progress\n",
    "            print(\"Epoch: (%d/%d) Validation [Loss: %f, Accuracy: %f]\"\n",
    "                  % (epoch, epochs - 1, v_loss, v_accuracy))\n",
    "        print(history.history.keys())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = SpeechCNNClassifier()\n",
    "\n",
    "# sTrain CNN\n",
    "batch_size = 8  # 32\n",
    "cnn.train(epochs=5, batch_size=batch_size)  # 25 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
