{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47d3bb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.convolutional import Conv2D, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Input, Dense, Reshape, Flatten, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from numpy.random import randn\n",
    "from numpy.random import choice\n",
    "# datapath and save could be out of a class and just functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c4bb780",
   "metadata": {},
   "outputs": [],
   "source": [
    "class wavPreprocessing:\n",
    "    \n",
    "    mel_spec_frame_size = 512 \n",
    "    highest_db = 80.0\n",
    "\n",
    "    def datapath_classes(datapath):\n",
    "        \"\"\"\n",
    "        using os.scandir is faster than os.listdir because the former is a generator\n",
    "        datapath:\n",
    "        Returns a list of sub-folders in the data path, assuming each subfolder is a separate class. Each sub-folder\n",
    "        is associated with a numeric class label, which is also returned.\n",
    "        :param datapath: Main data path\n",
    "        :param class_labels: labels per folder\n",
    "        :return: folder path, and the corresponding numeric class label for each folder\n",
    "        \"\"\"                    \n",
    "        subfolders = [f.path for f in os.scandir(datapath) if f.is_dir() and \"MACOS\" not in str(f)]  # ignoring macos on ubuntu and windows       \n",
    "        return [f.path for f in os.scandir(subfolders[0]) if f.is_dir()]   # fetching path with index\n",
    "    \n",
    "    def wavfiles(folder):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns the list of .wav files in a directory.\n",
    "        :param datapath: Directory to search for wav files in.\n",
    "        :return: List of paths to wav files.\n",
    "        \"\"\"              \n",
    "        return [f.path for f in os.scandir(folder) if f.path.endswith(\".wav\")]               \n",
    "    \n",
    "    def wav_to_melspec(wavfile, n_mels=64, plot=False):\n",
    "        \"\"\"\n",
    "        \"\"\"           \n",
    "        sig, fs = librosa.load(wavfile, sr=None)\n",
    "\n",
    "    # Normalize audio to between -1.0 and +1.0\n",
    "        sig /= np.max(np.abs(sig), axis=0)\n",
    "        n_fft = wavPreprocessing.mel_spec_frame_size\n",
    "        hop_length = int(n_fft/2)\n",
    "\n",
    "        if len(sig) < fs: # pad if less than a second\n",
    "            shape = np.shape(sig)\n",
    "            padded_array = np.zeros(fs)\n",
    "            padded_array[:shape[0]] = sig\n",
    "            sig = padded_array\n",
    "\n",
    "        melspec = librosa.feature.melspectrogram(y=sig,\n",
    "                                                 sr=fs,\n",
    "                                                 center=True,\n",
    "                                                 n_fft=n_fft,\n",
    "                                                 hop_length=hop_length,\n",
    "                                                 n_mels=n_mels)\n",
    "\n",
    "        if plot:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Mel-Frequency')\n",
    "            librosa.display.specshow(librosa.power_to_db(melspec, ref=np.max),\n",
    "                                     y_axis='mel',\n",
    "                                     fmax=fs/2,\n",
    "                                     sr=fs,\n",
    "                                     hop_length=hop_length,\n",
    "                                     x_axis='time')\n",
    "            plt.colorbar(format='%+2.0f dB')\n",
    "            plt.title('Mel spectrogram')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        melspec = librosa.power_to_db(melspec, ref=1.0)\n",
    "        melspec = melspec / wavPreprocessing.highest_db # scale by max dB\n",
    "\n",
    "        return melspec, fs   \n",
    "    \n",
    "    def check_melspec(melspec, fs, path):\n",
    "        \n",
    "        if melspec.shape[1] < 64:  # the melspec size is wrong\n",
    "            shape = np.shape(melspec)\n",
    "            padded_array = np.zeros((shape[0],64))-1\n",
    "            padded_array[0:shape[0],:shape[1]] = melspec\n",
    "            melspec = padded_array\n",
    "        saving = StoreMelspec(melspec, fs, path)\n",
    "        saving.save_melspec()                \n",
    "        return melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21beaa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructAudio(wavPreprocessing):\n",
    "    def __init__(self, melspec, fs, folder, file):          \n",
    "        self.melspec = melspec\n",
    "        self.fs = fs\n",
    "        self.folder = folder\n",
    "        self.file = file        \n",
    "        self.highest_db = super().highest_db  \n",
    "        self.mel_spec_frame_size = super().mel_spec_frame_size        \n",
    "    \n",
    "    def reconstruct(self, plot=False):\n",
    "        self.melspec = self.melspec[:, :-1]\n",
    "        signal = self.reconstruct_wav(plot=plot)\n",
    "        try:\n",
    "            os.mkdir(os.path.join(self.folder, 'recon'))\n",
    "        except FileExistsError:\n",
    "            pass  # should it be a return None? or do we want it to still run?\n",
    "        recon_wav_filename = (os.path.join(self.folder, 'recon', self.file))        \n",
    "        #scipy.io.wavfile.write(recon_wav_filename, self.fs, signal)\n",
    "        scipy.io.wavfile.write(recon_wav_filename, self.fs, signal.astype(np.float32)) #  must state type to read on windows\n",
    "    def reconstruct_wav(self, plot):\n",
    "        \"\"\"\n",
    "        Given a mel-spectrogram, and a target sampling frequency, reconstructs audio.\n",
    "        :param melspec: Mel-spectrogram, np.ndarray [shape=(n_mels, t)]\n",
    "        :param fs: Sampling frequency\n",
    "        :param plot: Flag to either plot the wav or not.\n",
    "        :return: The reconstructed raw samples of an audio signal.\n",
    "        \"\"\"        \n",
    "    #convert normed image back to dB range\n",
    "        self.melspec *= self.highest_db\n",
    "\n",
    "    # convert db back to power\n",
    "        self.melspec = librosa.db_to_power(self.melspec)\n",
    "\n",
    "        sig = librosa.feature.inverse.mel_to_audio(M=self.melspec,\n",
    "                                                   sr=self.fs,\n",
    "                                                   center=True,\n",
    "                                                   n_fft=self.mel_spec_frame_size,\n",
    "                                                   hop_length=int(self.mel_spec_frame_size/2))\n",
    "\n",
    "    # Normalize audio to between -1.0 and +1.0\n",
    "        sig /= np.max(np.abs(sig), axis=0)\n",
    "\n",
    "        if len(sig) < self.fs: # pad if less than a second\n",
    "            shape = np.shape(sig)\n",
    "            padded_array = np.zeros(self.fs)\n",
    "            padded_array[:shape[0]] = sig\n",
    "            sig = padded_array\n",
    "\n",
    "        if plot:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.xlabel('Sample')\n",
    "            plt.ylabel('Amplitude')\n",
    "            plt.plot(sig)\n",
    "            plt.title('Waveform')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        return sig\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d04c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoreMelspec:\n",
    "    \n",
    "    def __init__(self, melspec, fs, path):\n",
    "        self.path = path\n",
    "        self.melspec = melspec\n",
    "        self.fs = fs\n",
    "    \n",
    "    def save_melspec(self):    \n",
    "\n",
    "                    \n",
    "        melspec_filename = (self.path.replace('.wav', '.mel'))        \n",
    "        np.savez(melspec_filename, melspec=self.melspec, fs=self.fs)\n",
    "\n",
    "        melspec_image_filename = (self.path.replace('.wav', '.png'))\n",
    "        fig = plt.figure(figsize=(10, 4))\n",
    "        plt.imshow(self.melspec, origin='lower')\n",
    "        plt.tight_layout()\n",
    "        self.save_image(filepath=melspec_image_filename, fig=fig)\n",
    "        plt.close()\n",
    "        \n",
    "    def save_image(self, filepath, fig=None):\n",
    "        '''Save the current image with no whitespace\n",
    "        Example filepath: \"myfig.png\" or r\"C:\\myfig.pdf\"\n",
    "        '''\n",
    "        if not fig:\n",
    "            fig = plt.gcf()\n",
    "\n",
    "        plt.subplots_adjust(0,0,1,1,0,0)\n",
    "        for ax in fig.axes:\n",
    "            ax.axis('off')\n",
    "            ax.margins(0,0)\n",
    "            ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "            ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "        fig.savefig(filepath, pad_inches = 0, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04f9240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_preprocessing(preprocessing, path=\"./assignment_data\"):    \n",
    "    dirs = preprocessing.datapath_classes(path)    \n",
    "    for d in dirs:\n",
    "        wave_files = preprocessing.wavfiles(d)\n",
    "        for wav in wave_files:\n",
    "            melspec, fs = preprocessing.wav_to_melspec(wav)                                      \n",
    "            melspec = preprocessing.check_melspec(melspec, fs, wav)                      \n",
    "            audio_recons = ReconstructAudio(melspec, fs, d, os.path.normpath(os.path.basename(wav)))\n",
    "            audio_recons.reconstruct()        \n",
    "\n",
    "need_to_preprocess = None\n",
    "need_to_preprocess = True\n",
    "if need_to_preprocess == False:\n",
    "    wav_preprocessing(wavPreprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b2eabe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_set_files():\n",
    "    \"\"\"\n",
    "    Returns a list of training data files (NPZ files)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    subfolders = wavPreprocessing.datapath_classes(\"./assignment_data\")\n",
    "    return [f.path for folder in subfolders for f in os.scandir(folder) if f.path.endswith(\".npz\")]    # training images\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a6f3046",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = training_set_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b748a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(StoreMelspec): \n",
    "    # https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/\n",
    "    def __init__(self, training_set):\n",
    "        # Input shape        \n",
    "        self.img_rows = 64\n",
    "        self.img_cols = 64\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 32\n",
    "        self.trainingset = training_set\n",
    "\n",
    "        optimizer = Adam(0.0001, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator(depth=16)\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator(depth=64)        \n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))    \n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False        \n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        # debugging  \n",
    "        valid = self.discriminator(img)                \n",
    "        \n",
    "        # The combined model (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    # generate points in latent space as input for the generator\n",
    "    def generate_latent_points(self, latent_dim, n_samples):\n",
    "        noise = np.random.normal(0, 1, (n_samples, self.latent_dim))\n",
    "        return noise\n",
    "\n",
    "    def build_generator(self, depth=64):\n",
    "        \"\"\"\n",
    "        Defines a Generator network.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Dense(4 * 4 * int(depth*4), input_shape=(self.latent_dim,)))\n",
    "        model.add(Reshape((4, 4, int(depth*4))))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(int(depth*4), kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(int(depth*4), kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(int(depth*4), kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(1, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "        \n",
    "        model.summary()\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "        \n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self, depth=16):\n",
    "        \"\"\"\n",
    "        Defines a Discriminator network.\n",
    "        :return:\n",
    "        \"\"\"        \n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(int(depth), kernel_size=3, strides=1, padding=\"same\", input_shape=self.img_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(1, activation=\"sigmoid\"))\n",
    "        model.summary()\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)        \n",
    "        return Model(img, validity)  # img, validity\n",
    "\n",
    "    def train(self, epochs, batch_size=64):\n",
    "\n",
    "        half_batch = int(batch_size/2)\n",
    "\n",
    "        # Load the dataset and shuffle it\n",
    "        X_train = np.asarray(self.trainingset)\n",
    "        np.take(X_train, np.random.permutation(X_train.shape[0]), axis=0, out=X_train)\n",
    "\n",
    "        # steps per epoch\n",
    "        steps_per_epoch = int(X_train.shape[0]/half_batch)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for step in range(steps_per_epoch):\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "                # Adversarial ground truths\n",
    "                valid = np.ones((half_batch, 1))\n",
    "                fake = np.zeros((half_batch, 1))\n",
    "\n",
    "                # Select next batch of images (and shuffle indexes)\n",
    "                idx = np.arange(step*half_batch,(step*half_batch)+half_batch)                \n",
    "                np.take(idx, np.random.permutation(idx.shape[0]), axis=0, out=idx)\n",
    "                imgs = []\n",
    "                for file_path in X_train[idx]:                     \n",
    "                    npzfile = np.load(file_path)\n",
    "                    melspec = npzfile['melspec']                    \n",
    "                    imgs.append(melspec)                \n",
    "                imgs = np.asarray(imgs)\n",
    "                imgs = np.expand_dims(imgs, axis=3)\n",
    "\n",
    "\n",
    "                # Sample noise and generate a batch of new images\n",
    "                noise = self.generate_latent_points(self.latent_dim, half_batch)\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "                # Train the discriminator (real classified as ones and generated as zeros)      \n",
    "                d_loss_real, d_acc_real = self.discriminator.train_on_batch(imgs, valid)                \n",
    "                d_loss_fake, d_acc_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                d_acc = 0.5 * np.add(d_acc_real, d_acc_fake)\n",
    "\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                # Train the generator (wants discriminator to mistake images as real)\n",
    "                # Sample noise and generate a batch of new images\n",
    "                noise = self.generate_latent_points(self.latent_dim, batch_size)\n",
    "                valid = np.ones((batch_size, 1))\n",
    "                g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "                # Plot the progress\n",
    "                print(\"Epoch: (%d/%d) Step: (%d/%d) [D: loss_R: %f, loss_F: %f, loss: %f, acc_R: %.2f%%, acc_F: %.2f%%, acc.: %.2f%%] [G: loss: %f]\"\n",
    "                       % (epoch, epochs-1, step, steps_per_epoch-1, d_loss_real, d_loss_fake, d_loss, d_acc_real*100, d_acc_fake*100, d_acc*100, g_loss))\n",
    "\n",
    "            # Save generated image samples at every epoch\n",
    "            self.save_imgs(epoch)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        samples = 5\n",
    "        noise = self.generate_latent_points(self.latent_dim, samples)\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        for sample_idx in range(0, samples):\n",
    "            melspec = gen_imgs[sample_idx, :,:,0]\n",
    "\n",
    "            fig = plt.figure(figsize=(10, 4))\n",
    "            plt.imshow(melspec, origin='lower')\n",
    "            plt.tight_layout()  # check where sav_image method is\n",
    "            try:\n",
    "                os.mkdir(\"images\")  # develop images in a new dir? reconsider\n",
    "            except FileExistsError:\n",
    "                pass\n",
    "            \n",
    "            super().save_image(filepath=(\"images/epoch_%d_sample_%d.png\" % (epoch,sample_idx)), fig=fig)\n",
    "            #save_image(filepath=(\"images/epoch_%d_sample_%d.png\" % (epoch,sample_idx)), fig=fig)\n",
    "            plt.close()\n",
    "\n",
    "            \n",
    "            # wavwrite method handled by audio reconstruct\n",
    "            audio_epoch_recons = ReconstructAudio(melspec, fs=16000, folder=\"images\", \n",
    "                                                  file=\"epoch_%d_sample_%d.wav\" % (epoch,sample_idx))\n",
    "            audio_epoch_recons.reconstruct()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "596eb6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 12:49:59.941430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-05-20 12:50:00.564134: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-05-20 12:50:00.564160: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-05-20 12:50:00.597038: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 16)        160       \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 64, 64, 16)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 65536)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 65536)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65537     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,697\n",
      "Trainable params: 65,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 4096)              135168    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 8, 8, 256)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 16, 16, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " up_sampling2d_2 (UpSampling  (None, 32, 32, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 256)       590080    \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " up_sampling2d_3 (UpSampling  (None, 64, 64, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 64, 64, 1)         2305      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 64, 64, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,907,713\n",
      "Trainable params: 1,907,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch: (0/4) Step: (0/2) [D: loss_R: 0.633386, loss_F: 0.693789, loss: 0.663588, acc_R: 75.00%, acc_F: 75.00%, acc.: 75.00%] [G: loss: 0.702712]\n",
      "Epoch: (0/4) Step: (1/2) [D: loss_R: 0.335583, loss_F: 0.811064, loss: 0.573324, acc_R: 100.00%, acc_F: 0.00%, acc.: 50.00%] [G: loss: 0.579627]\n",
      "Epoch: (0/4) Step: (2/2) [D: loss_R: 0.225181, loss_F: 1.151373, loss: 0.688277, acc_R: 100.00%, acc_F: 0.00%, acc.: 50.00%] [G: loss: 0.461554]\n",
      "Epoch: (1/4) Step: (0/2) [D: loss_R: 0.222907, loss_F: 1.396244, loss: 0.809575, acc_R: 100.00%, acc_F: 0.00%, acc.: 50.00%] [G: loss: 0.458388]\n",
      "Epoch: (1/4) Step: (1/2) [D: loss_R: 0.327058, loss_F: 1.079012, loss: 0.703035, acc_R: 100.00%, acc_F: 0.00%, acc.: 50.00%] [G: loss: 0.748902]\n",
      "Epoch: (1/4) Step: (2/2) [D: loss_R: 0.459679, loss_F: 0.538806, loss: 0.499242, acc_R: 100.00%, acc_F: 100.00%, acc.: 100.00%] [G: loss: 1.395355]\n",
      "Epoch: (2/4) Step: (0/2) [D: loss_R: 0.627227, loss_F: 0.318952, loss: 0.473090, acc_R: 75.00%, acc_F: 100.00%, acc.: 87.50%] [G: loss: 1.552436]\n",
      "Epoch: (2/4) Step: (1/2) [D: loss_R: 0.689958, loss_F: 0.410475, loss: 0.550216, acc_R: 75.00%, acc_F: 100.00%, acc.: 87.50%] [G: loss: 1.263871]\n",
      "Epoch: (2/4) Step: (2/2) [D: loss_R: 0.524967, loss_F: 0.571409, loss: 0.548188, acc_R: 100.00%, acc_F: 100.00%, acc.: 100.00%] [G: loss: 0.935022]\n",
      "Epoch: (3/4) Step: (0/2) [D: loss_R: 0.395739, loss_F: 0.670553, loss: 0.533146, acc_R: 100.00%, acc_F: 100.00%, acc.: 100.00%] [G: loss: 0.714395]\n",
      "Epoch: (3/4) Step: (1/2) [D: loss_R: 0.394753, loss_F: 0.747171, loss: 0.570962, acc_R: 100.00%, acc_F: 0.00%, acc.: 50.00%] [G: loss: 0.637524]\n",
      "Epoch: (3/4) Step: (2/2) [D: loss_R: 0.239858, loss_F: 0.812604, loss: 0.526231, acc_R: 100.00%, acc_F: 0.00%, acc.: 50.00%] [G: loss: 0.657153]\n",
      "Epoch: (4/4) Step: (0/2) [D: loss_R: 0.140088, loss_F: 0.733220, loss: 0.436654, acc_R: 100.00%, acc_F: 0.00%, acc.: 50.00%] [G: loss: 0.709749]\n",
      "Epoch: (4/4) Step: (1/2) [D: loss_R: 0.118328, loss_F: 0.760474, loss: 0.439401, acc_R: 100.00%, acc_F: 0.00%, acc.: 50.00%] [G: loss: 0.638919]\n",
      "Epoch: (4/4) Step: (2/2) [D: loss_R: 0.098450, loss_F: 0.925609, loss: 0.512030, acc_R: 100.00%, acc_F: 0.00%, acc.: 50.00%] [G: loss: 0.530641]\n"
     ]
    }
   ],
   "source": [
    "dcgan = DCGAN(training_set=train_set[:15])\n",
    "dcgan.train(epochs=5, batch_size=8)  # 500 and 32\n",
    "#train_set[:15]  CHECK LAST CELL TO INHERIT SAV METHOD AND RECONSTRUCT METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eddf079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
