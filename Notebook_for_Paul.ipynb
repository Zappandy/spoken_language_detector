{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1ca4036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torchaudio\n",
    "from torch.utils.data import Subset\n",
    "#import torchaudio.transforms as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "from torch import cuda\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a80a6f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/mnt/c/Users/noral/Documents/M2_TAL/NN/spoken_language_detector/myowntest')\n",
    "os.mkdir('model_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ce528",
   "metadata": {},
   "source": [
    "Dataloader (SpeechDataset class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2fb7eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "\n",
    "    def __init__(self, flac_dir, load_method):\n",
    "        self.audio_path_list = sorted(self.find_files(flac_dir))  # do we need them to be sorted?\n",
    "        methods = {\"librosa\": self.librosa_flac2melspec, \"soundfile\": self.sf_loader, \"torchaudio\": self.torch_flac2melspec}\n",
    "        self.labels = {\"es\": 0, \"en\": 1, \"de\": 2}\n",
    "        self.languages = {v: k for k, v in self.labels.items()}\n",
    "        self.chosen_method = methods[load_method]\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_path_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_file = self.audio_path_list[index]  \n",
    "        label = self.labels[self.get_label(audio_file)]\n",
    "        spectro, _ = self.chosen_method(audio_file)  # _ is fs\n",
    "        #spectro = torch.from_numpy(spectro)\n",
    "        #spectro = spectro.unsqueeze(0)\n",
    "        return spectro, label\n",
    "\n",
    "    def find_files(self, directory, pattern=\".flac\"):\n",
    "        \"\"\"\n",
    "        Recursive search method to find files. Credit to Paul Magron and Andrea de Marco\n",
    "        for OG method\n",
    "        \"\"\"\n",
    "\n",
    "        regexFlac = re.compile(rf'/.*fragment\\d+\\{pattern}$') \n",
    "        files = [f.path for f in os.scandir(directory) if regexFlac.search(f.path)]  # ends with does not like regex\n",
    "        return files\n",
    "    \n",
    "    def monolingual_path_list(self, language, gender):\n",
    "        \"\"\"\n",
    "        Input: string ('en'|'de'|'es')\n",
    "        Output: audio path list only containing file names of the chosen language\n",
    "        \"\"\"\n",
    "        #r = re.compile(rf'.*\\/{language}_{gender}.*fragment\\d+\\.flac') \n",
    "        r = re.compile(rf'.*\\/{language}.*fragment\\d+\\.flac') \n",
    "        newlist = list(filter(r.match, self.audio_path_list))\n",
    "        return newlist\n",
    "    \n",
    "    def get_label(self, path):\n",
    "        labelRegex = re.compile(r\"(es|en|de)_.*.flac\")\n",
    "        patterns = labelRegex.findall(path)\n",
    "        assert len(patterns) == 1\n",
    "        return patterns[0]\n",
    "\n",
    "    def torch_flac2melspec(self, file_path):\n",
    "        pass  # commenting option to avoid pip issues on colab\n",
    "        #waveform, sample_rate = torchaudio.load(file_path, normalize=True)\n",
    "        #transform = T.MelSpectrogram(sample_rate)        \n",
    "        #return transform(waveform), sample_rate\n",
    "        \n",
    "    def sf_loader(self, file_path):\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            data, samplerate = sf.read(f)\n",
    "        return data, samplerate\n",
    "\n",
    "    def librosa_flac2melspec(self, file_path, n_mels=64, melspec_size=512, visual=False):\n",
    "        \"\"\"\n",
    "        the librosa method we are using atm\n",
    "        \"\"\"\n",
    "        sig, fs =  librosa.load(file_path, sr=None)\n",
    "        sig /= np.max(np.abs(sig), axis=0)\n",
    "        n_fft = melspec_size\n",
    "        hop_length = int(n_fft/2)\n",
    "\n",
    "        # padding signal if less than a second\n",
    "        if len(sig) < fs:\n",
    "            padded_array = np.zeros(fs)\n",
    "            padded_array[:np.shape(sig)[0]] = sig\n",
    "            sig = padded_array\n",
    "\n",
    "        melspec = librosa.feature.melspectrogram(y=sig, sr=fs,\n",
    "                                                 center=True, n_fft=n_fft,\n",
    "                                                 hop_length=hop_length, n_mels=n_mels)\n",
    "\n",
    "\n",
    "        if visual:\n",
    "            self.plotmelspec(melspec, fs, hop_length)\n",
    "\n",
    "        melspec = librosa.power_to_db(melspec, ref=1.0)\n",
    "        melspec /= 80.0  # highest db...\n",
    "        melspec = self.checkmelspec(melspec)\n",
    "        return melspec, fs\n",
    "\n",
    "    def checkmelspec(self, melspec, n_mels=64):\n",
    "        \"\"\"\n",
    "        this method works with librosa\n",
    "\n",
    "        \"\"\"\n",
    "        if melspec.shape[1] < n_mels:  # n_mels\n",
    "            shape = np.shape(melspec)\n",
    "            padded_array = np.zeros((shape[0], n_mels)) - 1\n",
    "            padded_array[0:shape[0], :shape[1]] = melspec\n",
    "            melspec = padded_array\n",
    "        return melspec\n",
    "\n",
    "    def plotmelspec(self, melspec, fs, hop_length, show=False):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Mel-Frequency\")\n",
    "        librosa.display.specshow(librosa.power_to_db(melspec, ref=np.max),\n",
    "                                 y_axis=\"mel\", fmax=fs/2, sr=fs,\n",
    "                                 hop_length=hop_length, x_axis=\"time\")\n",
    "        plt.colorbar(format=\"%+2.0f db\")\n",
    "        plt.title(\"Mel Spectogram\")\n",
    "        plt.tight_layout()\n",
    "        if show:\n",
    "            plt.show()\n",
    "        plt.close()  # to close windows and fix warning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51545b98",
   "metadata": {},
   "source": [
    "Function to get balanced subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "afedb590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced_subset(train_data, n, gender):\n",
    "    '''\n",
    "    Creates subset with equal amounts of data from each language\n",
    "    Input: instance of class Speechdataset, desired number of files per language (int) \n",
    "    Output: balanced subset (torch.utils.data.dataset.ConcatDataset)\n",
    "    '''\n",
    "    #create new instances for each language\n",
    "    en=deepcopy(train_data)\n",
    "    es=deepcopy(train_data)\n",
    "    de=deepcopy(train_data)\n",
    "    \n",
    "    \n",
    "    #modify the audio_path_list to only include paths to files of a single language\n",
    "    en.audio_path_list=en.monolingual_path_list('en', gender)\n",
    "    es.audio_path_list=es.monolingual_path_list('es', gender)\n",
    "    de.audio_path_list=de.monolingual_path_list('de', gender)\n",
    "\n",
    "    #extract equally sized subsets from each monolingual dataset\n",
    "    en_sub = Subset(en, torch.arange(n))\n",
    "    es_sub = Subset(es, torch.arange(n))\n",
    "    de_sub = Subset(de, torch.arange(n))\n",
    "    \n",
    "    subset=en_sub+de_sub+es_sub #concatenate subsets\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd140ea",
   "metadata": {},
   "source": [
    "utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b39232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://debuggercafe.com/saving-and-loading-the-best-model-in-pytorch/\n",
    "\n",
    "def evaluation(model, val_data, loss_fn):\n",
    "\n",
    "    device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        loss = 0\n",
    "        total = 0\n",
    "        for spectra, labels in val_data:\n",
    "\n",
    "            spectra = spectra.unsqueeze(1)\n",
    "            spectra = spectra.to(device)\n",
    "            labels = labels.to(device)\n",
    "            preds = model(spectra)\n",
    "            vals, labels_preds = torch.max(preds.data, 1)  # preds.data == preds? vals are not needed\n",
    "            total += labels.size(0)  # same as shape[0], what's more pytorch-like?\n",
    "            correct += (labels_preds == labels).sum().item()\n",
    "            # loss\n",
    "            err = loss_fn(preds, labels)\n",
    "            loss += err.item()\n",
    "        total_loss = loss / len(val_data)\n",
    "    return correct / total * 100, total_loss\n",
    "\n",
    "def visualize(epochs, tr_loss, val_loss, save=False):\n",
    "\n",
    "    fig = plt.figure()\n",
    "    train, = plt.plot(torch.arange(epochs) + 1, tr_loss, '-og', label=\"Train\")  \n",
    "    valid, = plt.plot(torch.arange(epochs) + 1, val_loss, '-or', label=\"Valid\")  \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(\"Loss\")    \n",
    "    plt.legend(handles=[train, valid])\n",
    "    plt.title('Loss over epochs')\n",
    "    if save:\n",
    "        fig.savefig(\"Loss_over_epochs.jpg\", bbox_inches=\"tight\", dpi=150)\n",
    "    #plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def save_model(epochs, model, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Function to save the trained model to disk.\n",
    "    \"\"\"\n",
    "    print(f\"Saving final model...\")\n",
    "    torch.save({\n",
    "                'epoch': epochs,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': criterion,\n",
    "                }, 'model_output/final_speech_cnn.pth')\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Class to save the best model while training. If the current epoch's \n",
    "    validation loss is less than the previous least less, then save the\n",
    "    model state.\n",
    "    \"\"\"\n",
    "    #def __init__(self, best_valid_loss=float('inf')):\n",
    "    def __init__(self, best_valid_loss=float('inf')):\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        #self.path = \"./model_output/best_speech_cnn.pth\"\n",
    "        self.path = \"model_output/best_speech_cnn.pth\"\n",
    "        \n",
    "    def __call__(self, current_valid_loss, epoch, model, optimizer, criterion):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            torch.save({'epoch': epoch+1, 'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': criterion}, self.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f5f1d9",
   "metadata": {},
   "source": [
    "CNNSpeechClassifier Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1261333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/runtimeerror-given-groups-1-weight-64-3-3-3-so-expected-input-16-64-256-256-to-have-3-channels-but-got-64-channels-instead/12765/3\n",
    "# https://www.google.com/search?q=RuntimeError%3A+Given+groups%3D1%2C+weight+of+size+%5B16%2C+1%2C+2%2C+2%5D%2C+expected+input%5B1%2C+8%2C+64%2C+862%5D+to+have+1+channels%2C+but+got+8+channels+&source=hp&ei=5aisY9HLEaPh7_UP7eaXaA&iflsig=AJiK0e8AAAAAY6y29Zezs58YNZGiLVUa9aIDcDbq9cY0&ved=0ahUKEwiR0pKhlZ38AhWj8LsIHW3zBQ0Q4dUDCAg&uact=5&oq=RuntimeError%3A+Given+groups%3D1%2C+weight+of+size+%5B16%2C+1%2C+2%2C+2%5D%2C+expected+input%5B1%2C+8%2C+64%2C+862%5D+to+have+1+channels%2C+but+got+8+channels+&gs_lcp=Cgdnd3Mtd2l6EANQAFgAYO8BaABwAHgAgAEAiAEAkgEAmAEAoAECoAEB&sclient=gws-wiz\n",
    "class CNNSpeechClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, channel_inputs, num_channels1, num_channels2, kernel_size, kernel_pool, padding, num_classes):\n",
    "        num_channels3 = 64\n",
    "        super(CNNSpeechClassifier, self).__init__()\n",
    "        self.cnn_layer1 = nn.Sequential(nn.Conv2d(channel_inputs, num_channels1, kernel_size=kernel_size, padding=padding),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.BatchNorm2d(num_channels1),\n",
    "                                        nn.MaxPool2d(kernel_pool))\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc_layer = nn.Linear(num_channels1*31*430, num_classes)  # shape of cnn_layer 1 after convolution of image!\n",
    "        #self.cnn_layer2 = nn.Sequential(nn.Conv2d(num_channels1, num_channels2, kernel_size=kernel_size, padding=padding),\n",
    "        #                                nn.ReLU(),\n",
    "        #                                nn.BatchNorm2d(num_channels2),\n",
    "        #                                nn.MaxPool2d(kernel_pool))\n",
    "\n",
    "        ##self.fc_layer = nn.Linear(num_channels2*15*214, num_classes)  # shape of cnn_layer 2 after convolution of image!\n",
    "\n",
    "        #self.cnn_layer3 = nn.Sequential(nn.Conv2d(num_channels2, num_channels3, kernel_size=kernel_size, padding=padding),\n",
    "        #                                nn.ReLU(),\n",
    "        #                                nn.BatchNorm2d(num_channels3),\n",
    "        #                                nn.MaxPool2d(kernel_pool))\n",
    "        #\n",
    "\n",
    "        #self.fc_layer = nn.Linear(num_channels3*7*106, num_classes)  # shape of cnn_layer 2 after convolution of image!\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1, 8, 64, 862 should be 8, 1, 64, 862\n",
    "        x = self.cnn_layer1(x)\n",
    "        #print(x.shape)\n",
    "        #x = self.cnn_layer2(x)\n",
    "        ##print(x.shape)\n",
    "        #x = self.cnn_layer3(x)\n",
    "        #print(x.shape)\n",
    "        #raise SystemExit\n",
    "        # vectorizing image\n",
    "        z = x.reshape(x.shape[0], -1)\n",
    "        z = self.dropout(z)\n",
    "        return self.fc_layer(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46863ddb",
   "metadata": {},
   "source": [
    "train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8494314",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer:\n",
    "\n",
    "    def __init__(self, model, lr=1e-6):\n",
    "\n",
    "        self.device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.total_train_loss = []\n",
    "        self.total_val_loss = []\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.model = copy.deepcopy(model)\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "        lr = 1e-4  # 1e-4 best so far?\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def train_loop(self, train_data, val_data, epochs=10, verbose=True, visual=False):\n",
    "\n",
    "        save_best_model = SaveBestModel()\n",
    "\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            loss_curr_epoch = 0\n",
    "            for spectra, labels in train_data:\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                spectra = spectra.unsqueeze(1)\n",
    "\n",
    "                spectra = spectra.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                preds = self.model(spectra)  # 8, 3\n",
    "                loss = self.loss_fn(preds, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                loss_curr_epoch += loss.item()\n",
    "\n",
    "            train_loss = loss_curr_epoch / len(train_data)\n",
    "            self.total_train_loss.append(train_loss)\n",
    "            acc, val_loss = evaluation(self.model, val_data, self.loss_fn)\n",
    "            self.total_val_loss.append(val_loss)\n",
    "            if verbose:\n",
    "                self.pretty_print(epoch=epoch, train_loss=train_loss, val_loss=val_loss, acc=acc)\n",
    "\n",
    "            save_best_model(val_loss, epoch, self.model, self.optimizer, self.loss_fn)\n",
    "\n",
    "\n",
    "        \n",
    "        if visual:\n",
    "            visualize(epochs, self.total_train_loss, self.total_val_loss)\n",
    "        save_model(epochs=epochs, model=self.model, optimizer=self.optimizer, criterion=self.loss_fn)\n",
    "    \n",
    "    def pretty_print(self, epoch, train_loss, val_loss, acc):\n",
    "        print(f\"Epoch {epoch+1}: train loss is {train_loss:.3f} | val loss is {val_loss:.3f} | Accuracy is {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110ce7f",
   "metadata": {},
   "source": [
    "eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8fed5c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "def load_components(checkpoint):\n",
    "\n",
    "\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    loss_fn = checkpoint[\"loss\"]\n",
    "    #loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model = CNNSpeechClassifier(channel_inputs=1, num_channels1=16,\n",
    "                                num_channels2=32, kernel_size=2,\n",
    "                                kernel_pool=2, padding=0, num_classes=3)\n",
    "\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "    optimizer = AdamW(model.parameters())\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    return model, optimizer, epoch, loss_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b3f86e",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a43bfb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_output/best_speech_cnn.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m test_data \u001b[38;5;241m=\u001b[39m get_balanced_subset(test_data, \u001b[38;5;241m80\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#TODO: redefine logic to only create subsets based on gender?\u001b[39;00m\n\u001b[1;32m      6\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(test_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# one is 4, 64, 862 - 4 despite batch size\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m best_checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_output/best_speech_cnn.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice)  \u001b[38;5;66;03m# weird form of early stopping. Should add patience\u001b[39;00m\n\u001b[1;32m      9\u001b[0m final_checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_output/final_speech_cnn.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     10\u001b[0m model, optimizer, epoch, loss_fn \u001b[38;5;241m=\u001b[39m load_components(best_checkpoint)\n",
      "File \u001b[0;32m/mnt/c/Users/noral/Documents/M2_TAL/Install/nn-env/lib/python3.10/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/mnt/c/Users/noral/Documents/M2_TAL/Install/nn-env/lib/python3.10/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/mnt/c/Users/noral/Documents/M2_TAL/Install/nn-env/lib/python3.10/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_output/best_speech_cnn.pth'"
     ]
    }
   ],
   "source": [
    "#test_dir = \"Dataset/test/test\"\n",
    "test_dir = \"/mnt/c/Users/noral/Documents/M2_TAL/NN/Project/test/test\"\n",
    "\n",
    "test_data = SpeechDataset(test_dir, \"librosa\")\n",
    "test_data = get_balanced_subset(test_data, 80, 'f') #TODO: redefine logic to only create subsets based on gender?\n",
    "test_dataloader = DataLoader(test_data, batch_size=8, shuffle=True)  # one is 4, 64, 862 - 4 despite batch size\n",
    "\n",
    "best_checkpoint = torch.load(\"model_output/best_speech_cnn.pth\", map_location=device)  # weird form of early stopping. Should add patience\n",
    "final_checkpoint = torch.load(\"model_output/final_speech_cnn.pth\", map_location=device)\n",
    "model, optimizer, epoch, loss_fn = load_components(best_checkpoint)\n",
    "\n",
    "acc, test_loss = evaluation(model, test_dataloader, loss_fn)\n",
    "\n",
    "print(f\"Best epoch {epoch}: test loss is {test_loss:.3f} | Accuracy is {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd76e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d429f1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c232eaf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
